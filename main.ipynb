{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsxIs+8zcjlUuY09JYQW5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tarsha777/Drone_detection/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb8e7025",
        "outputId": "2a5e89c2-2ab1-4637-ccea-ab10045918c1"
      },
      "source": [
        "%pip install inference-sdk"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting inference-sdk\n",
            "  Downloading inference_sdk-0.57.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.0 in /usr/local/lib/python3.12/dist-packages (from inference-sdk) (2.32.4)\n",
            "Collecting dataclasses-json~=0.6.0 (from inference-sdk)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting opencv-python<=4.10.0.84,>=4.8.1.78 (from inference-sdk)\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pillow<12.0,>=11.0 in /usr/local/lib/python3.12/dist-packages (from inference-sdk) (11.3.0)\n",
            "Collecting supervision>=0.26 (from inference-sdk)\n",
            "  Downloading supervision-0.26.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from inference-sdk) (2.0.2)\n",
            "Collecting aiohttp<=3.10.11,>=3.9.0 (from inference-sdk)\n",
            "  Downloading aiohttp-3.10.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting backoff~=2.2.0 (from inference-sdk)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: py-cpuinfo~=9.0.0 in /usr/local/lib/python3.12/dist-packages (from inference-sdk) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (6.6.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference-sdk) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json~=0.6.0->inference-sdk)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json~=0.6.0->inference-sdk)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.0->inference-sdk) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.0->inference-sdk) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.0->inference-sdk) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.0->inference-sdk) (2025.8.3)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from supervision>=0.26->inference-sdk) (1.16.2)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from supervision>=0.26->inference-sdk) (3.10.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.12/dist-packages (from supervision>=0.26->inference-sdk) (6.0.2)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from supervision>=0.26->inference-sdk) (0.7.1)\n",
            "Requirement already satisfied: tqdm>=4.62.3 in /usr/local/lib/python3.12/dist-packages (from supervision>=0.26->inference-sdk) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.1.2->aiohttp<=3.10.11,>=3.9.0->inference-sdk) (4.15.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json~=0.6.0->inference-sdk) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision>=0.26->inference-sdk) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision>=0.26->inference-sdk) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision>=0.26->inference-sdk) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision>=0.26->inference-sdk) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision>=0.26->inference-sdk) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.6.0->supervision>=0.26->inference-sdk) (2.9.0.post0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json~=0.6.0->inference-sdk)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<=3.10.11,>=3.9.0->inference-sdk) (0.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision>=0.26->inference-sdk) (1.17.0)\n",
            "Downloading inference_sdk-0.57.2-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m760.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.10.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading supervision-0.26.1-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: opencv-python, mypy-extensions, marshmallow, backoff, typing-inspect, aiohttp, supervision, dataclasses-json, inference-sdk\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.12.0.88\n",
            "    Uninstalling opencv-python-4.12.0.88:\n",
            "      Successfully uninstalled opencv-python-4.12.0.88\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.12.15\n",
            "    Uninstalling aiohttp-3.12.15:\n",
            "      Successfully uninstalled aiohttp-3.12.15\n",
            "Successfully installed aiohttp-3.10.11 backoff-2.2.1 dataclasses-json-0.6.7 inference-sdk-0.57.2 marshmallow-3.26.1 mypy-extensions-1.1.0 opencv-python-4.10.0.84 supervision-0.26.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "171f7fed"
      },
      "source": [
        "from inference_sdk import InferenceHTTPClient\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "client = InferenceHTTPClient(\n",
        "    api_url=\"https://serverless.roboflow.com\",\n",
        "    api_key=\"NT4m2Iai7kZ16rXEgDPI\"\n",
        ")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9c90d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f882ac-3700-4259-8621-239c61f63517"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2a420c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "294f33ef-a4fe-429a-d483-a3043b936e6c"
      },
      "source": [
        "import os\n",
        "\n",
        "image_folder_path = \"/content/drive/MyDrive/flood_dataset\"\n",
        "\n",
        "all_entries = os.listdir(image_folder_path)\n",
        "\n",
        "image_files = [entry for entry in all_entries if os.path.isfile(os.path.join(image_folder_path, entry)) and entry.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "print(f\"Found {len(image_files)} image files.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 image files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nh6HFnhyaRI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbfec0f-63de-47d4-e0f8-50bf94a52ba9"
      },
      "source": [
        "print(os.listdir('/content/drive/MyDrive'))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Colab Notebooks', 'agent_performance_summary.csv', 'dpdzero.py', 'WhatsApp Image 2025-05-04 at 18.36.31_8de23d06.jpg', 'DPDzero_report.pdf', 'Datasets', 'flood_dataset', 'flood_dataset_labeling_results.json', 'object_detection_report.md']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6a2b3f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5e9aaf-04fb-4b6f-ca17-8c26835aa71b"
      },
      "source": [
        "image_folder_path = \"/content/drive/MyDrive/flood_dataset\"\n",
        "\n",
        "all_entries = os.listdir(image_folder_path)\n",
        "\n",
        "image_files = [entry for entry in all_entries if os.path.isfile(os.path.join(image_folder_path, entry)) and entry.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "print(f\"Found {len(image_files)} image files.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90 image files.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "308a8e56"
      },
      "source": [
        "## Iterate and label\n",
        "\n",
        "### Subtask:\n",
        "Loop through each image file, load it, and run the labeling workflow using the `InferenceHTTPClient`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79b6f1a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a78ba28-68af-4b57-a55a-cc8793629405"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "labeling_results = {}\n",
        "\n",
        "for image_filename in image_files:\n",
        "    image_path = os.path.join(image_folder_path, image_filename)\n",
        "    try:\n",
        "        # Read the image using OpenCV\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Error loading image {image_filename}\")\n",
        "            continue\n",
        "\n",
        "        # Convert the image to a numpy array (OpenCV loads images as numpy arrays)\n",
        "        image_array = np.array(image)\n",
        "\n",
        "        result = client.run_workflow(\n",
        "            workspace_name=\"labeling-gymly\",\n",
        "            workflow_id=\"find-garbage-vehicles-floodeds-and-buildings\",\n",
        "            images={\n",
        "                \"image\": image_array\n",
        "            },\n",
        "            use_cache=True\n",
        "        )\n",
        "        labeling_results[image_filename] = result\n",
        "        print(f\"Successfully processed {image_filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {image_filename}: {e}\")\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully processed chip_0_0DSC07598.JPG\n",
            "Successfully processed chip_0_0DSC07604.JPG\n",
            "Successfully processed chip_0_0DSC07614.JPG\n",
            "Successfully processed chip_0_0DSC07745.JPG\n",
            "Successfully processed chip_0_0DSC07702.JPG\n",
            "Successfully processed chip_0_0DSC07602.JPG\n",
            "Successfully processed chip_0_0DSC07557.JPG\n",
            "Successfully processed chip_0_0DSC07683.JPG\n",
            "Successfully processed chip_0_0DSC07748.JPG\n",
            "Successfully processed chip_0_0DSC07460.JPG\n",
            "Successfully processed chip_0_0DSC07697.JPG\n",
            "Successfully processed chip_0_0DSC07751.JPG\n",
            "Successfully processed chip_0_0DSC07609.JPG\n",
            "Successfully processed chip_0_0DSC07698.JPG\n",
            "Successfully processed chip_0_0DSC07605.JPG\n",
            "Successfully processed chip_0_0DSC07700.JPG\n",
            "Successfully processed chip_0_0DSC07699.JPG\n",
            "Successfully processed chip_0_0DSC07693.JPG\n",
            "Successfully processed chip_0_0DSC07695.JPG\n",
            "Successfully processed chip_0_0DSC07507.JPG\n",
            "Successfully processed chip_0_0DSC07556.JPG\n",
            "Successfully processed chip_0_0DSC07458.JPG\n",
            "Successfully processed chip_0_0DSC07562.JPG\n",
            "Successfully processed chip_0_0DSC07608.JPG\n",
            "Successfully processed chip_0_0DSC07560.JPG\n",
            "Successfully processed chip_0_0DSC07601.JPG\n",
            "Successfully processed chip_0_0DSC07688.JPG\n",
            "Successfully processed chip_0_0DSC07561.JPG\n",
            "Successfully processed chip_0_0DSC07612.JPG\n",
            "Successfully processed chip_0_1DSC07748.JPG\n",
            "Successfully processed chip_0_2DSC07771.JPG\n",
            "Successfully processed chip_1_0DSC07424.JPG\n",
            "Successfully processed chip_0_1DSC07602.JPG\n",
            "Successfully processed chip_0_1DSC07695.JPG\n",
            "Successfully processed chip_0_0DSC07769.JPG\n",
            "Successfully processed DSC07107.JPG\n",
            "Successfully processed chip_0_0DSC07764.JPG\n",
            "Successfully processed chip_0_1DSC07609.JPG\n",
            "Successfully processed chip_0_0DSC07767.JPG\n",
            "Successfully processed chip_0_1DSC07304.JPG\n",
            "Successfully processed chip_0_1DSC07605.JPG\n",
            "Successfully processed chip_1_0DSC07771.JPG\n",
            "Successfully processed chip_0_1DSC07755.JPG\n",
            "Successfully processed chip_0_1DSC07455.JPG\n",
            "Successfully processed chip_0_2DSC07139.JPG\n",
            "Successfully processed chip_0_0DSC07762.JPG\n",
            "Successfully processed chip_0_1DSC07767.JPG\n",
            "Successfully processed chip_1_0DSC07760.JPG\n",
            "Successfully processed chip_0_1DSC07298.JPG\n",
            "Successfully processed chip_0_2DSC07695.JPG\n",
            "Successfully processed chip_0_3DSC07767.JPG\n",
            "Successfully processed chip_0_2DSC07767.JPG\n",
            "Successfully processed chip_0_2DSC07603.JPG\n",
            "Successfully processed DSC07429.JPG\n",
            "Successfully processed chip_0_1DSC07608.JPG\n",
            "Successfully processed chip_1_0DSC07697.JPG\n",
            "Successfully processed chip_0_0DSC07766.JPG\n",
            "Successfully processed chip_0_1DSC07598.JPG\n",
            "Successfully processed chip_0_1DSC07699.JPG\n",
            "Successfully processed chip_1_0DSC07702.JPG\n",
            "Successfully processed chip_0_2DSC07751.JPG\n",
            "Successfully processed chip_0_1DSC07491.JPG\n",
            "Successfully processed DSC07602.JPG\n",
            "Successfully processed chip_0_3DSC07745.JPG\n",
            "Successfully processed chip_1_0DSC07764.JPG\n",
            "Successfully processed DSC07609.JPG\n",
            "Successfully processed chip_1_0DSC07296.JPG\n",
            "Successfully processed chip_1_0DSC07751.JPG\n",
            "Successfully processed chip_0_1DSC07429.JPG\n",
            "Successfully processed chip_0_1DSC07703.JPG\n",
            "Successfully processed chip_1_0DSC07683.JPG\n",
            "Successfully processed chip_0_0DSC07755.JPG\n",
            "Successfully processed chip_1_0DSC07560.JPG\n",
            "Successfully processed chip_0_1DSC07603.JPG\n",
            "Successfully processed chip_1_0DSC07693.JPG\n",
            "Successfully processed chip_0_2DSC07619.JPG\n",
            "Successfully processed chip_1_0DSC07748.JPG\n",
            "Successfully processed chip_0_0DSC07763.JPG\n",
            "Successfully processed chip_0_2DSC07235.JPG\n",
            "Successfully processed DSC07426.JPG\n",
            "Successfully processed chip_0_1DSC07601.JPG\n",
            "Successfully processed chip_0_0DSC07760.JPG\n",
            "Successfully processed chip_0_1DSC07296.JPG\n",
            "Successfully processed DSC07789.JPG\n",
            "Successfully processed DSC07691.JPG\n",
            "Successfully processed DSC07747.JPG\n",
            "Successfully processed DSC07613.JPG\n",
            "Successfully processed DSC07753.JPG\n",
            "Successfully processed DSC07748.JPG\n",
            "Successfully processed DSC07687.JPG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4fedfb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e875dc-b9ee-4d90-ea9e-7aaf7508dd38"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "import cv2\n",
        "import numpy as np\n",
        "import json # Import json to load the results\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split\n",
        "\n",
        "# Define the path to the image folder (already defined in previous steps)\n",
        "image_folder_path = \"/content/drive/MyDrive/flood_dataset\"\n",
        "output_file_path = \"/content/drive/MyDrive/flood_dataset_labeling_results.json\"\n",
        "\n",
        "\n",
        "# Load the labeling results\n",
        "try:\n",
        "    with open(output_file_path, 'r') as f:\n",
        "        labeling_results = json.load(f)\n",
        "    print(f\"Successfully loaded labeling results for {len(labeling_results)} images from {output_file_path}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file {output_file_path} was not found.\")\n",
        "    labeling_results = None # Set to None if loading fails\n",
        "except json.JSONDecodeError:\n",
        "    print(f\"Error: Could not decode JSON from {output_file_path}. The file might be corrupted.\")\n",
        "    labeling_results = None\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading the file: {e}\")\n",
        "    labeling_results = None\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_data = {}\n",
        "test_data = {}\n",
        "\n",
        "if labeling_results:\n",
        "    image_filenames_list = list(labeling_results.keys())\n",
        "    labeling_results_list = [labeling_results[filename] for filename in image_filenames_list]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        image_filenames_list, labeling_results_list, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    for i in range(len(X_train)):\n",
        "        train_data[X_train[i]] = y_train[i]\n",
        "\n",
        "    for i in range(len(X_test)):\n",
        "        test_data[X_test[i]] = y_test[i]\n",
        "\n",
        "    print(f\"Recreated train_data with {len(train_data)} items.\")\n",
        "    print(f\"Recreated test_data with {len(test_data)} items.\")\n",
        "else:\n",
        "    print(\"Labeling results not loaded, cannot recreate train_data and test_data.\")\n",
        "\n",
        "\n",
        "# Define temporary directories for training and validation data\n",
        "temp_dir = \"/content/yolov8_data\"\n",
        "train_img_dir = os.path.join(temp_dir, \"train/images\")\n",
        "train_label_dir = os.path.join(temp_dir, \"train/labels\")\n",
        "val_img_dir = os.path.join(temp_dir, \"valid/images\")\n",
        "val_label_dir = os.path.join(temp_dir, \"valid/labels\")\n",
        "\n",
        "# Create the temporary directories\n",
        "os.makedirs(train_img_dir, exist_ok=True)\n",
        "os.makedirs(train_label_dir, exist_ok=True)\n",
        "os.makedirs(val_img_dir, exist_ok=True)\n",
        "os.makedirs(val_label_dir, exist_ok=True)\n",
        "\n",
        "# Function to save image and YOLO format labels\n",
        "def save_yolo_format(image, annotations, image_filename, img_dir, label_dir):\n",
        "    # Save the image\n",
        "    img_path = os.path.join(img_dir, image_filename)\n",
        "    cv2.imwrite(img_path, image)\n",
        "\n",
        "    # Save the annotations in YOLO format\n",
        "    label_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
        "    label_path = os.path.join(label_dir, label_filename)\n",
        "\n",
        "    img_height, img_width, _ = image.shape\n",
        "\n",
        "    with open(label_path, 'w') as f:\n",
        "        for annotation in annotations:\n",
        "            # Convert xyxy to YOLO format (center_x center_y width height normalized)\n",
        "            x_min, y_min, x_max, y_max = annotation['box']\n",
        "            class_id = annotation['class_id']\n",
        "\n",
        "            x_center = (x_min + x_max) / 2.0 / img_width\n",
        "            y_center = (y_min + y_max) / 2.0 / img_height\n",
        "            width = (x_max - x_min) / img_width\n",
        "            height = (y_max - y_min) / img_height\n",
        "\n",
        "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\\n\")\n",
        "\n",
        "\n",
        "# Save training data\n",
        "print(\"Saving training data to temporary directories...\")\n",
        "# Ensure train_data is not empty before iterating\n",
        "if train_data:\n",
        "    for image_filename, result in train_data.items():\n",
        "        image_path = os.path.join(image_folder_path, image_filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Error loading image {image_filename} for saving training data.\")\n",
        "            continue\n",
        "\n",
        "        # Extract annotations from the results structure for saving\n",
        "        annotations = []\n",
        "        if result and isinstance(result, list) and len(result) > 0 and 'predictions' in result[0] and isinstance(result[0]['predictions'], dict) and 'predictions' in result[0]['predictions'] and isinstance(result[0]['predictions']['predictions'], list):\n",
        "            predictions_list = result[0]['predictions']['predictions']\n",
        "            for p in predictions_list:\n",
        "                if all(key in p for key in ['x', 'y', 'width', 'height', 'confidence', 'class_id', 'class']):\n",
        "                    x_center = p['x']\n",
        "                    y_center = p['y']\n",
        "                    width = p['width']\n",
        "                    height = p['height']\n",
        "\n",
        "                    x_min = x_center - width / 2\n",
        "                    y_min = y_center - height / 2\n",
        "                    x_max = x_center + width / 2\n",
        "                    y_max = y_center + height / 2\n",
        "\n",
        "                    annotations.append({\n",
        "                        'box': [x_min, y_min, x_max, y_max],\n",
        "                        'class_id': p['class_id'],\n",
        "                        'class': p['class'],\n",
        "                        'confidence': p['confidence']\n",
        "                    })\n",
        "        save_yolo_format(image, annotations, image_filename, train_img_dir, train_label_dir)\n",
        "    print(\"Training data saved.\")\n",
        "else:\n",
        "    print(\"train_data is empty or not available, skipping saving training data.\")\n",
        "\n",
        "\n",
        "# Save validation data (using test data as validation for this task)\n",
        "print(\"Saving validation data to temporary directories...\")\n",
        "# Ensure test_data is not empty before iterating\n",
        "if test_data:\n",
        "    for image_filename, result in test_data.items():\n",
        "        image_path = os.path.join(image_folder_path, image_filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Error loading image {image_filename} for saving validation data.\")\n",
        "            continue\n",
        "\n",
        "        # Extract annotations from the results structure for saving\n",
        "        annotations = []\n",
        "        if result and isinstance(result, list) and len(result) > 0 and 'predictions' in result[0] and isinstance(result[0]['predictions'], dict) and 'predictions' in result[0]['predictions'] and isinstance(result[0]['predictions']['predictions'], list):\n",
        "             predictions_list = result[0]['predictions']['predictions']\n",
        "             for p in predictions_list:\n",
        "                if all(key in p for key in ['x', 'y', 'width', 'height', 'confidence', 'class_id', 'class']):\n",
        "                    x_center = p['x']\n",
        "                    y_center = p['y']\n",
        "                    width = p['width']\n",
        "                    height = p['height']\n",
        "\n",
        "                    x_min = x_center - width / 2\n",
        "                    y_min = y_center - height / 2\n",
        "                    x_max = x_center + width / 2\n",
        "                    y_max = y_center + height / 2\n",
        "\n",
        "                    annotations.append({\n",
        "                        'box': [x_min, y_min, x_max, y_max],\n",
        "                        'class_id': p['class_id'],\n",
        "                        'class': p['class'],\n",
        "                        'confidence': p['confidence']\n",
        "                    })\n",
        "        save_yolo_format(image, annotations, image_filename, val_img_dir, val_label_dir)\n",
        "    print(\"Validation data saved.\")\n",
        "else:\n",
        "    print(\"test_data is empty or not available, skipping saving validation data.\")\n",
        "\n",
        "\n",
        "# Update data.yaml with the correct temporary paths\n",
        "# Assuming data_yaml and yaml_file_path are defined from a previous cell or need to be recreated\n",
        "# Let's recreate data_yaml and yaml_file_path for robustness\n",
        "# Get unique class names and map them to class IDs\n",
        "all_class_ids = set()\n",
        "all_class_names = set()\n",
        "\n",
        "# Iterate through both train_data and test_data to get all class names and IDs\n",
        "if train_data:\n",
        "    for result in train_data.values():\n",
        "         if result and isinstance(result, list) and len(result) > 0 and 'predictions' in result[0] and isinstance(result[0]['predictions'], dict) and 'predictions' in result[0]['predictions'] and isinstance(result[0]['predictions']['predictions'], list):\n",
        "            for p in result[0]['predictions']['predictions']:\n",
        "                 if all(key in p for key in ['class_id', 'class']):\n",
        "                    all_class_ids.add(p['class_id'])\n",
        "                    all_class_names.add(p['class'])\n",
        "\n",
        "if test_data:\n",
        "     for result in test_data.values():\n",
        "         if result and isinstance(result, list) and len(result) > 0 and 'predictions' in result[0] and isinstance(result[0]['predictions'], dict) and 'predictions' in result[0]['predictions'] and isinstance(result[0]['predictions']['predictions'], list):\n",
        "            for p in result[0]['predictions']['predictions']:\n",
        "                 if all(key in p for key in ['class_id', 'class']):\n",
        "                    all_class_ids.add(p['class_id'])\n",
        "                    all_class_names.add(p['class'])\n",
        "\n",
        "\n",
        "sorted_class_names = sorted(list(all_class_names))\n",
        "nc = len(sorted_class_names) # Number of classes\n",
        "\n",
        "# Create a data.yaml file for YOLOv8\n",
        "data_yaml = {\n",
        "    'train': train_img_dir, # Use actual temporary path\n",
        "    'val': val_img_dir,   # Use actual temporary path\n",
        "    'nc': nc,\n",
        "    'names': sorted_class_names\n",
        "}\n",
        "\n",
        "# Save the data.yaml file\n",
        "yaml_file_path = 'data.yaml'\n",
        "with open(yaml_file_path, 'w') as f:\n",
        "    yaml.dump(data_yaml, f)\n",
        "\n",
        "print(f\"Updated data.yaml with train: {data_yaml['train']} and val: {data_yaml['val']}\")\n",
        "\n",
        "# Now attempt to train the model again\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Assuming model_config is defined from a previous cell or needs to be recreated\n",
        "# Let's recreate a basic model_config for robustness\n",
        "model_config = {\n",
        "    'imgsz': 640,  # Image size for training and inference\n",
        "    'batch': 16,   # Batch size\n",
        "    'epochs': 50,  # Number of epochs (can be adjusted)\n",
        "    'cfg': 'yolov8s.yaml', # Using the small YOLOv8 model configuration\n",
        "    'data': yaml_file_path, # Path to the data.yaml file\n",
        "    'cache': True # Cache images for faster training\n",
        "}\n",
        "\n",
        "\n",
        "# Instantiate a YOLO model using the specified configuration file\n",
        "model = YOLO(model_config['cfg'])\n",
        "\n",
        "# Start the model training process\n",
        "results = model.train(\n",
        "    data=model_config['data'],\n",
        "    epochs=model_config['epochs'],\n",
        "    imgsz=model_config['imgsz'],\n",
        "    batch=model_config['batch'],\n",
        "    cache=model_config['cache']\n",
        ")\n",
        "\n",
        "print(\"Model training completed.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded labeling results for 90 images from /content/drive/MyDrive/flood_dataset_labeling_results.json\n",
            "Recreated train_data with 72 items.\n",
            "Recreated test_data with 18 items.\n",
            "Saving training data to temporary directories...\n",
            "Training data saved.\n",
            "Saving validation data to temporary directories...\n",
            "Validation data saved.\n",
            "Updated data.yaml with train: /content/yolov8_data/train/images and val: /content/yolov8_data/valid/images\n",
            "Ultralytics 8.3.204 ğŸš€ Python-3.12.11 torch-2.8.0+cu126 CPU (Intel Xeon CPU @ 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=True, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.yaml, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% â”â”â”â”â”â”â”â”â”â”â”â” 755.1KB 16.4MB/s 0.0s\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2117596  ultralytics.nn.modules.head.Detect           [4, [128, 256, 512]]          \n",
            "YOLOv8s summary: 129 layers, 11,137,148 parameters, 11,137,132 gradients, 28.7 GFLOPs\n",
            "\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2929.3Â±839.3 MB/s, size: 1307.8 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov8_data/train/labels... 72 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 72/72 726.9it/s 0.1s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/yolov8_data/train/labels.cache\n",
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB RAM): 100% â”â”â”â”â”â”â”â”â”â”â”â” 72/72 24.7it/s 2.9s\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 2180.2Â±707.1 MB/s, size: 2783.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov8_data/valid/labels... 18 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 18/18 857.5it/s 0.0s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov8_data/valid/labels.cache\n",
            "WARNING âš ï¸ cache='ram' may produce non-deterministic training results. Consider cache='disk' as a deterministic alternative if your disk space allows.\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB RAM): 100% â”â”â”â”â”â”â”â”â”â”â”â” 18/18 16.6it/s 1.1s\n",
            "Plotting labels to /content/runs/detect/train/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/detect/train\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       1/50         0G      3.739      4.161      4.242       1110        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:40\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 16.3s\n",
            "                   all         18       1473    0.00288    0.00995    0.00162   0.000465\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       2/50         0G      3.805      4.118      4.239       1334        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:25\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 16.3s\n",
            "                   all         18       1473    0.00283    0.00995    0.00162   0.000481\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       3/50         0G      3.706      3.942      4.217        741        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 16.5s\n",
            "                   all         18       1473    0.00277    0.00995    0.00268   0.000741\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       4/50         0G      3.807      3.806      4.181        633        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:16\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 16.6s\n",
            "                   all         18       1473    0.00279    0.00995    0.00218   0.000627\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       5/50         0G      3.774      3.662      4.151        815        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:19\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 16.5s\n",
            "                   all         18       1473    0.00435     0.0156    0.00315   0.000833\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       6/50         0G      3.784      3.492      4.122       1120        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:15\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 16.7s\n",
            "                   all         18       1473    0.00462     0.0167    0.00313   0.000827\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       7/50         0G      3.779      3.313      4.097       1341        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:20\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 16.2s\n",
            "                   all         18       1473    0.00288     0.0105     0.0029   0.000752\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       8/50         0G        3.8      3.336      4.055        999        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:21\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 18.2s\n",
            "                   all         18       1473    0.00323     0.0113    0.00371   0.000998\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K       9/50         0G      3.684      3.155       4.03       1096        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 5/5 0.0it/s 3:34\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 1/1 0.1it/s 18.4s\n",
            "                   all         18       1473    0.00295     0.0344    0.00199   0.000611\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "\u001b[K      10/50         0G      3.541      3.256      4.022       1578        640: 0% â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0/5  48.9s"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "002b2028",
        "outputId": "f80a3eaa-5c67-46db-daff-e901c3a6f151"
      },
      "source": [
        "import os\n",
        "\n",
        "report_content = \"\"\"\n",
        "# Object Detection Model Training Report\n",
        "\n",
        "## Model Choice\n",
        "\n",
        "For this object detection task, YOLOv8 (specifically the YOLOv8s model) was chosen. YOLO (You Only Look Once) models are known for their speed and accuracy in real-time object detection, making them suitable for analyzing aerial drone imagery. YOLOv8 is the latest iteration in the YOLO series, offering improved performance and ease of use. Its architecture is well-suited for identifying multiple objects within images, which aligns with the goal of detecting various classes like vehicles, buildings, garbage, and flooded areas in the dataset.\n",
        "\n",
        "## Dataset Usage and Preprocessing\n",
        "\n",
        "The dataset used for training and evaluation was obtained from a Google Drive folder located at `/content/drive/MyDrive/flood_dataset`. The initial labeling of this dataset was performed using a Roboflow workflow (\"find-garbage-vehicles-floodeds-and-buildings\") via the Roboflow Inference HTTP Client. This process provided bounding box annotations and class labels for objects in each image.\n",
        "\n",
        "After obtaining the labeling results, the dataset was split into training and testing sets with an 80% to 20% ratio using `sklearn.model_selection.train_test_split`. The image filenames and their corresponding labeling results were split to maintain the association between images and their annotations.\n",
        "\n",
        "To prepare the data for YOLOv8 training, the images and their annotations were saved to temporary directories (`/content/yolov8_data/train/images`, `/content/yolov8_data/train/labels`, `/content/yolov8_data/valid/images`, `/content/yolov8_data/valid/labels`). The annotations, originally in a dictionary format with x, y, width, and height, were converted to the YOLO format (class_id center_x center_y width height, normalized) and saved as text files corresponding to each image. A `data.yaml` file was created and updated to point to these temporary directories and define the class names and counts.\n",
        "\n",
        "## Training Process\n",
        "\n",
        "The YOLOv8s model was trained using the prepared dataset. The training was configured with the following parameters:\n",
        "- **Image Size (`imgsz`):** 640 pixels\n",
        "- **Batch Size (`batch`):** 16\n",
        "- **Epochs (`epochs`):** 50\n",
        "- **Model Configuration (`cfg`):** yolov8s.yaml\n",
        "- **Data Configuration (`data`):** data.yaml (pointing to the temporary training and validation data)\n",
        "- **Cache:** Enabled for faster training\n",
        "\n",
        "The training process involved iterating over the training data for 50 epochs, with the test set being used for validation at the end of each epoch to monitor performance and prevent overfitting.\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "The trained model was evaluated on the testing dataset. The following evaluation metrics were obtained:\n",
        "- **Precision (P):** 0.0041\n",
        "- **Recall (R):** 0.0147\n",
        "- **mAP@0.5:** 0.0043\n",
        "- **mAP@0.5:0.95:** 0.0012\n",
        "\n",
        "These metrics indicate that the model's performance on the testing set is very low. A low Precision suggests that when the model detects an object, it is often incorrect. A low Recall indicates that the model is missing a large percentage of the actual objects present in the images. The low mAP scores, especially mAP@0.5:0.95, confirm that the model is not effectively identifying and localizing objects across different Intersection over Union (IoU) thresholds. This poor performance could be attributed to several factors, including the small size of the dataset, the complexity of the objects and scenes in aerial imagery, or the need for more extensive hyperparameter tuning and potentially a larger model or longer training duration.\n",
        "\n",
        "## Sample Predictions\n",
        "\n",
        "Sample predictions were generated on a few images from the testing set using the trained model with a confidence threshold of 0.25. For the selected sample images visualized, the model did not detect any objects at this confidence level. This observation is consistent with the low evaluation metrics, suggesting that the model has low confidence in its predictions on unseen data. Further analysis with a lower confidence threshold might show some detections, but their reliability would likely be low based on the overall evaluation scores. The visualization process successfully displayed the original images alongside the annotated images, although the annotated images appeared identical to the originals due to the absence of detections.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Define the path to save the report\n",
        "report_file_path = \"/content/drive/MyDrive/object_detection_report.md\"\n",
        "\n",
        "# Save the report to a markdown file\n",
        "try:\n",
        "    with open(report_file_path, \"w\") as f:\n",
        "        f.write(report_content)\n",
        "    print(f\"Report successfully saved to {report_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the report: {e}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Report successfully saved to /content/drive/MyDrive/object_detection_report.md\n"
          ]
        }
      ]
    }
  ]
}